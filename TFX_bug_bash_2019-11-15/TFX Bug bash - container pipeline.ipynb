{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TFX Bug bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_endpoint = None\n",
    "\n",
    "# ! Use kfp.Client(host='https://xxxxx.notebooks.googleusercontent.com/') if working from GCP notebooks (or local notebooks)\n",
    "cluster_endpoint = 'https://34c40cdd21e49f0a-dot-us-central1.notebooks.googleusercontent.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "_pipeline_name = 'tfx_container_pipeline'\n",
    "#_tfx_root = os.path.join(os.environ['HOME'], 'tfx')\n",
    "_tfx_root = 'gs://avolkov/tmp/tfx/' + _pipeline_name\n",
    "\n",
    "_pipeline_root = os.path.join(_tfx_root, 'pipelines', _pipeline_name)\n",
    "# Sqlite ML-metadata db path.\n",
    "_metadata_path = os.path.join(_tfx_root, 'metadata', _pipeline_name, 'metadata.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pip install --upgrade 'tfx>=0.15.0' --user --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "import tfx\n",
    "#import tfx.types\n",
    "\n",
    "from tfx.components.base import base_component, executor_spec\n",
    "from tfx.types import component_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFX cannot use types that are not partt of the base image: AttributeError: module '__main__' has no attribute 'CsvArtifact'\n",
    "class CsvArtifact(component_spec.Artifact):\n",
    "    TYPE_NAME = 'CsvPath'\n",
    "\n",
    "class QueryBigQueryComponent(base_component.BaseComponent):\n",
    "    class Spec(component_spec.ComponentSpec):\n",
    "        INPUTS = {}\n",
    "        OUTPUTS = {\n",
    "            #'results': component_spec.ChannelParameter(type=CsvArtifact), # Does not work with custom types\n",
    "            'results': component_spec.ChannelParameter(type_name='CsvPath'), # Using this deprecated method to workaround TFX deficiencies\n",
    "        }\n",
    "        PARAMETERS = {\n",
    "            'query': component_spec.ExecutionParameter(type=str),\n",
    "        }\n",
    "\n",
    "    SPEC_CLASS = Spec\n",
    "    EXECUTOR_SPEC = executor_spec.ExecutorContainerSpec(\n",
    "        image='google/cloud-sdk:latest',\n",
    "        command=[\n",
    "            'bash',\n",
    "            '-e',\n",
    "            '-x',\n",
    "            '-c',\n",
    "            '''\n",
    "            if [ -n \"$GOOGLE_APPLICATION_CREDENTIALS\" ]; then\n",
    "                gcloud auth activate-service-account --key-file=$GOOGLE_APPLICATION_CREDENTIALS\n",
    "            fi\n",
    "            query=\"$0\"\n",
    "            #results_uri=$1\n",
    "            results_uri=\"${1}data\"  # The passed path is a directory path. This is problematic since it forces people to add file name to that directory which will bring incompatibilities between components (e.g. components A and B write to /a.txt and /b.txt - the consumer does not know which file will exist) This problem has been fixed in KFP and passed paths are file paths (that can be used as directories though.)\n",
    "            results_path=$(mktemp)\n",
    "            \n",
    "            echo 'y' | bq init\n",
    "            bq query --nouse_legacy_sql --format csv -q \"$query\" > \"$results_path\"\n",
    "            if [ -n \"$results_uri\" ]; then\n",
    "                gsutil cp \"$results_path\" \"$results_uri\" #  The results_uri must not be a \"directory\" URI. Otherwise the uploaded blob name will be random: gs://avolkov/tmp/tfx/tfx_container_pipeline/pipelines/tfx_container_pipeline/QueryBigQueryComponent/results/87/tmp.Qh7VDPYFgx\n",
    "            fi\n",
    "            ''',\n",
    "            '{{exec_properties.query}}',  # Do not forget the commas!; \n",
    "            #'{{output_dict.results}}',  #='[Artifact(type_name: CsvPath, uri: gs://avolkov/tmp/tfx/tfx_container_pipeline/pipelines/tfx_container_pipeline/QueryBigQueryComponent/results/84/, split: , id: 0)]' ']'\n",
    "            #'{{output_dict.results.uri}}',  #=''\n",
    "            '{{output_dict.results[0].uri}}', #= 'gs://avolkov/tmp/tfx/tfx_container_pipeline/pipelines/tfx_container_pipeline/QueryBigQueryComponent/results/87/' - The path is a directory and this will force people adding file names to that directory which will bring incompatibilities between components (e.g. components A and B write to /a.txt and /b.txt - the consumer does not know which file will exist)\n",
    "        ])\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        query: str,\n",
    "        results: Optional[tfx.types.Channel] = None,\n",
    "        instance_name: str = 'QueryBigQueryComponent'\n",
    "    ):\n",
    "        results = results or tfx.types.Channel(\n",
    "            type=CsvArtifact,\n",
    "            artifacts=[\n",
    "        #        CsvArtifact(),\n",
    "                component_spec.Artifact(type_name='CsvPath'),\n",
    "            ],\n",
    "        )\n",
    "        super(QueryBigQueryComponent, self).__init__(\n",
    "            QueryBigQueryComponent.SPEC_CLASS(\n",
    "                query=query,\n",
    "                results=results,\n",
    "                instance_name=instance_name,\n",
    "            ),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFX cannot use types that are not partt of the base image: AttributeError: module '__main__' has no attribute 'CsvArtifact'\n",
    "class SKLearnSvmSvrModelArtifact(component_spec.Artifact):\n",
    "    TYPE_NAME = 'SKLearnSvmSvrModelPath'\n",
    "\n",
    "class SKLearnTransformerArtifact(component_spec.Artifact):\n",
    "    TYPE_NAME = 'SKLearnTransformerPath'\n",
    "\n",
    "#class FloatArtifact(component_spec.Artifact):\n",
    "#    TYPE_NAME = 'FloatPath'\n",
    "    \n",
    "\n",
    "class TrainSklearnSvmCsrComponent(base_component.BaseComponent):\n",
    "    class Spec(component_spec.ComponentSpec):\n",
    "        INPUTS = {\n",
    "            #'training_data': component_spec.ChannelParameter(type=CsvArtifact), # Does not work with custom types\n",
    "            'training_data': component_spec.ChannelParameter(type_name='CsvPath'), # Using this deprecated method to workaround TFX deficiencies\n",
    "        }\n",
    "        OUTPUTS = {\n",
    "            #'model': component_spec.ChannelParameter(type=SKLearnSvmSvrModel), # Does not work with custom types\n",
    "            'model': component_spec.ChannelParameter(type_name='SKLearnSvmSvrModelPath'), # Using this deprecated method to workaround TFX deficiencies\n",
    "            'transformer': component_spec.ChannelParameter(type_name='SKLearnTransformerPath'),\n",
    "            #'mean_square_error': component_spec.ChannelParameter(type_name='FloatPath'),\n",
    "        }\n",
    "        PARAMETERS = {\n",
    "            'target_column_name': component_spec.ExecutionParameter(type=str),\n",
    "        }\n",
    "\n",
    "    SPEC_CLASS = Spec\n",
    "    EXECUTOR_SPEC = executor_spec.ExecutorContainerSpec(\n",
    "        image='tensorflow/tensorflow:1.15.0-py3',\n",
    "        command=[\n",
    "            #'sh',\n",
    "            #'-e',\n",
    "            #'-x',\n",
    "            #'-c',\n",
    "            #'python3 -m pip install --upgrade --user pandas gcsfs && echo \"0=$0\" && echo \"1=$1\" && echo \"2=$2\" && echo \"3=$3\" && echo \"4=$4\" && \"$0\" \"$*\"',\n",
    "            'python3',\n",
    "            '-c',\n",
    "            '''\n",
    "import sys\n",
    "import subprocess\n",
    "subprocess.run([sys.executable, '-m', 'pip', 'install', '--upgrade', '--quiet', 'sklearn' , 'pandas', 'gcsfs'])\n",
    "\n",
    "\n",
    "training_data_uri = sys.argv[1]\n",
    "output_model_uri = sys.argv[2]\n",
    "output_transformer_uri = sys.argv[3]\n",
    "target_column_name = sys.argv[4]\n",
    "\n",
    "# The passed path is a directory path. This is problematic since it forces people to add file name to that directory which will bring incompatibilities between components (e.g. components A and B write to /a.txt and /b.txt - the consumer does not know which file will exist) This problem has been fixed in KFP and passed paths are file paths (that can be used as directories though.)\n",
    "if training_data_uri.endswith(\"/\"):\n",
    "    training_data_uri = training_data_uri + \"data\"\n",
    "if output_transformer_uri.endswith(\"/\"):\n",
    "    output_transformer_uri = output_transformer_uri + \"data\"\n",
    "if output_model_uri.endswith(\"/\"):\n",
    "    output_model_uri = output_model_uri + \"data\"\n",
    "\n",
    "import pandas\n",
    "data = pandas.read_csv(training_data_uri)\n",
    "cleaned_data = data.select_dtypes(\"number\").fillna(0)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "training_data, testing_data = train_test_split(cleaned_data, test_size=0.5)\n",
    "\n",
    "training_features = training_data.drop(target_column_name, axis=1).values\n",
    "training_labels = training_data[target_column_name].values\n",
    "testing_features = testing_data.drop(target_column_name, axis=1).values\n",
    "testing_labels = testing_data[target_column_name].values\n",
    "\n",
    "from sklearn import preprocessing\n",
    "transformer = preprocessing.StandardScaler()\n",
    "transformer.fit(training_features)\n",
    "\n",
    "scaled_training_features = transformer.transform(training_features)\n",
    "scaled_testing_features = transformer.transform(training_features)\n",
    "\n",
    "from sklearn import svm\n",
    "model = svm.SVR().fit(scaled_training_features, training_labels)\n",
    "\n",
    "predictions = model.predict(scaled_testing_features)\n",
    "\n",
    "from sklearn import metrics\n",
    "mean_squared_error = metrics.mean_squared_error(testing_labels, predictions)\n",
    "print(\"mean_squared_error=\" + str(mean_squared_error))\n",
    "\n",
    "import pickle\n",
    "def pickle_to_local_or_gcs(obj, path):\n",
    "    if path.startswith(\"gs:/\"):\n",
    "        import gcsfs\n",
    "        file = gcsfs.GCSFileSystem().open(path, \"wb\")\n",
    "    else:\n",
    "        file = open(path, \"wb\")\n",
    "\n",
    "    with file as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "pickle_to_local_or_gcs(transformer, output_transformer_uri)\n",
    "pickle_to_local_or_gcs(model, output_model_uri)\n",
    "''',\n",
    "            '{{input_dict.training_data[0].uri}}', #= 'gs://avolkov/tmp/tfx/tfx_container_pipeline/pipelines/tfx_container_pipeline/QueryBigQueryComponent/results/87/' - The path is a directory and this will force people adding file names to that directory which will bring incompatibilities between components (e.g. components A and B write to /a.txt and /b.txt - the consumer does not know which file will exist)\n",
    "            '{{output_dict.model[0].uri}}',\n",
    "            '{{output_dict.transformer[0].uri}}',\n",
    "            '{{exec_properties.target_column_name}}',  # Do not forget the commas!; \n",
    "        ])\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        target_column_name: str,\n",
    "        training_data: Optional[tfx.types.Channel] = None,\n",
    "        model: Optional[tfx.types.Channel] = None,\n",
    "        transformer: Optional[tfx.types.Channel] = None,\n",
    "        instance_name: str = 'TrainSklearnSvmCsr'\n",
    "    ):\n",
    "        training_data = training_data or tfx.types.Channel(\n",
    "            type=CsvArtifact,\n",
    "            artifacts=[\n",
    "        #        CsvArtifact(),\n",
    "                component_spec.Artifact(type_name='CsvPath'),\n",
    "            ],\n",
    "        )\n",
    "        model = model or tfx.types.Channel(\n",
    "            type=SKLearnSvmSvrModelArtifact,\n",
    "            artifacts=[\n",
    "        #        SKLearnSvmSvrModelArtifact(),\n",
    "                component_spec.Artifact(type_name='SKLearnSvmSvrModelPath'),\n",
    "            ],\n",
    "        )\n",
    "        transformer = transformer or tfx.types.Channel(\n",
    "            type=SKLearnTransformerArtifact,\n",
    "            artifacts=[\n",
    "        #        SKLearnTransformerArtifact(),\n",
    "                component_spec.Artifact(type_name='SKLearnTransformerPath'),\n",
    "            ],\n",
    "        )\n",
    "        #mean_square_error = mean_square_error or tfx.types.Channel(\n",
    "        #    type=FloatArtifact,\n",
    "        #    artifacts=[\n",
    "        ##        FloatArtifact(),\n",
    "        #        component_spec.Artifact(type_name='FloatPath'),\n",
    "        #    ],\n",
    "        #)\n",
    "        super(TrainSklearnSvmCsrComponent, self).__init__(\n",
    "            TrainSklearnSvmCsrComponent.SPEC_CLASS(\n",
    "                target_column_name=target_column_name,\n",
    "                training_data=training_data,\n",
    "                model=model,\n",
    "                transformer=transformer,\n",
    "                #mean_square_error=mean_square_error,\n",
    "                instance_name=instance_name,\n",
    "            ),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tfx.orchestration import metadata\n",
    "#metadata_connection_config=metadata.sqlite_metadata_connection_config(metadata_path)\n",
    "\n",
    "\n",
    "# The rate at which to sample rows from the Chicago Taxi dataset using BigQuery.\n",
    "# The full taxi dataset is > 120M record.  In the interest of resource\n",
    "# savings and time, we've set the default for this example to be much smaller.\n",
    "# Feel free to crank it up and process the full dataset!\n",
    "#_query_sample_rate = 0.001  # Generate a 0.1% random sample.\n",
    "_query_sample_rate = 0.01  # Generate a 0.1% random sample.\n",
    "\n",
    "# This is the upper bound of FARM_FINGERPRINT in Bigquery (ie the max value of\n",
    "# signed int64).\n",
    "_max_int64 = '0x7FFFFFFFFFFFFFFF'\n",
    "\n",
    "# The query that extracts the examples from BigQuery.  The Chicago Taxi dataset\n",
    "# used for this example is a public dataset available on Google AI Platform.\n",
    "# https://console.cloud.google.com/marketplace/details/city-of-chicago-public-data/chicago-taxi-trips\n",
    "_query = \"\"\"\n",
    "         SELECT\n",
    "           pickup_community_area,\n",
    "           fare,\n",
    "           EXTRACT(MONTH FROM trip_start_timestamp) AS trip_start_month,\n",
    "           EXTRACT(HOUR FROM trip_start_timestamp) AS trip_start_hour,\n",
    "           EXTRACT(DAYOFWEEK FROM trip_start_timestamp) AS trip_start_day,\n",
    "           UNIX_SECONDS(trip_start_timestamp) AS trip_start_timestamp,\n",
    "           pickup_latitude,\n",
    "           pickup_longitude,\n",
    "           dropoff_latitude,\n",
    "           dropoff_longitude,\n",
    "           trip_miles,\n",
    "           pickup_census_tract,\n",
    "           dropoff_census_tract,\n",
    "           payment_type,\n",
    "           company,\n",
    "           trip_seconds,\n",
    "           dropoff_community_area,\n",
    "           tips\n",
    "         FROM `bigquery-public-data.chicago_taxi_trips.taxi_trips`\n",
    "         WHERE (ABS(FARM_FINGERPRINT(unique_key)) / {max_int64})\n",
    "           < {query_sample_rate}\"\"\".format(\n",
    "               max_int64=_max_int64, query_sample_rate=_query_sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tfx.orchestration import pipeline\n",
    "\n",
    "def _create_pipeline(\n",
    "    pipeline_name: str,\n",
    "    pipeline_root: str,\n",
    "    metadata_path: str,\n",
    "    query: str,\n",
    ") -> pipeline.Pipeline:\n",
    "\n",
    "    bq_query_task = QueryBigQueryComponent(query=query)\n",
    "    train_svm_task = TrainSklearnSvmCsrComponent(\n",
    "        training_data=bq_query_task.outputs['results'],\n",
    "        target_column_name='tips',\n",
    "    )\n",
    "\n",
    "    return pipeline.Pipeline(\n",
    "        pipeline_name=pipeline_name,\n",
    "        pipeline_root=pipeline_root,\n",
    "        components=[\n",
    "            bq_query_task,\n",
    "            train_svm_task,\n",
    "        ],\n",
    "        enable_cache=False,\n",
    "        metadata_connection_config=metadata.sqlite_metadata_connection_config(metadata_path),\n",
    "        additional_pipeline_args={},\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import absl\n",
    "absl.logging.set_verbosity(absl.logging.INFO)\n",
    "\n",
    "from tfx.orchestration.beam import beam_dag_runner\n",
    "from tfx.orchestration.config import pipeline_config, docker_component_config\n",
    "from tfx.orchestration.launcher import docker_component_launcher, in_process_component_launcher, kubernetes_component_launcher\n",
    "\n",
    "if False:\n",
    "    beam_dag_runner.BeamDagRunner(\n",
    "      config=pipeline_config.PipelineConfig(\n",
    "          supported_launcher_classes=[\n",
    "              docker_component_launcher.DockerComponentLauncher\n",
    "          ],\n",
    "          default_component_configs=[\n",
    "              docker_component_config.DockerComponentConfig(volumes=[\n",
    "                  '/home/jupyter/.config/gcloud:/root/.config/gcloud'\n",
    "              ])\n",
    "          ])).run(\n",
    "              _create_pipeline(\n",
    "                  pipeline_name=_pipeline_name,\n",
    "                  pipeline_root=_pipeline_root,\n",
    "                  metadata_path=_metadata_path,\n",
    "                  query=_query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kubernetes import client as k8s_client\n",
    "def use_gcp_secret():\n",
    "    secret_name = 'user-gcp-sa'\n",
    "    secret_volume_mount_path = '/secret/gcp-credentials'\n",
    "    secret_file_path_in_volume = '/' + secret_name + '.json'\n",
    "    volume_name = 'gcp-credentials-' + secret_name\n",
    "    volumes = [\n",
    "        k8s_client.V1Volume(\n",
    "            name=volume_name,\n",
    "            secret=k8s_client.V1SecretVolumeSource(secret_name=secret_name))\n",
    "    ]\n",
    "    containers = [\n",
    "        k8s_client.V1Container(\n",
    "            name='main',\n",
    "            volume_mounts=[\n",
    "                k8s_client.V1VolumeMount(\n",
    "                    name=volume_name,\n",
    "                    mount_path=secret_volume_mount_path,\n",
    "                )\n",
    "            ],\n",
    "            env=[\n",
    "                k8s_client.V1EnvVar(\n",
    "                    name='GOOGLE_APPLICATION_CREDENTIALS',\n",
    "                    value=secret_volume_mount_path + secret_file_path_in_volume,\n",
    "                ),\n",
    "                k8s_client.V1EnvVar(\n",
    "                    name='CLOUDSDK_AUTH_CREDENTIAL_FILE_OVERRIDE',\n",
    "                    value=secret_volume_mount_path + secret_file_path_in_volume,\n",
    "                )\n",
    "            ])\n",
    "    ]\n",
    "\n",
    "    return k8s_client.V1Pod(\n",
    "        spec=k8s_client.V1PodSpec(\n",
    "            containers=containers,\n",
    "            volumes=volumes,\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Adding upstream dependencies for component QueryBigQueryComponent\n",
      "INFO:absl:Adding upstream dependencies for component TrainSklearnSvmCsrComponent\n",
      "INFO:absl:   ->  Component: QueryBigQueryComponent\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Experiment link <a href=\"https://34c40cdd21e49f0a-dot-us-central1.notebooks.googleusercontent.com/#/experiments/details/64288191-6f27-4804-ac6a-2629fd0f1f09\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run link <a href=\"https://34c40cdd21e49f0a-dot-us-central1.notebooks.googleusercontent.com/#/runs/details/386d35c7-8228-4f16-b8d4-71cf7c8c6dc7\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "RunPipelineResult(run_id=386d35c7-8228-4f16-b8d4-71cf7c8c6dc7)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tfx.orchestration.config import kubernetes_component_config\n",
    "from tfx.orchestration.kubeflow import kubeflow_dag_runner\n",
    "\n",
    "metadata_config = kubeflow_dag_runner.get_default_kubeflow_metadata_config()\n",
    "# This pipeline automatically injects the Kubeflow TFX image if the\n",
    "# environment variable 'KUBEFLOW_TFX_IMAGE' is defined. Currently, the tfx\n",
    "# cli tool exports the environment variable to pass to the pipelines.\n",
    "tfx_image = os.environ.get('KUBEFLOW_TFX_IMAGE', None)\n",
    "k8s_config = kubernetes_component_config.KubernetesComponentConfig(\n",
    "      use_gcp_secret(),\n",
    ")\n",
    "runner_config = kubeflow_dag_runner.KubeflowDagRunnerConfig(\n",
    "    kubeflow_metadata_config=metadata_config,\n",
    "    # Specify custom docker image to use.\n",
    "    tfx_image=tfx_image,\n",
    "    supported_launcher_classes=[\n",
    "        in_process_component_launcher.InProcessComponentLauncher,\n",
    "        kubernetes_component_launcher.KubernetesComponentLauncher,\n",
    "    ],\n",
    "    default_component_configs=[k8s_config])\n",
    "\n",
    "kubeflow_dag_runner.KubeflowDagRunner(config=runner_config).run(\n",
    "    _create_pipeline(\n",
    "        pipeline_name=_pipeline_name,\n",
    "        pipeline_root=_pipeline_root,\n",
    "        metadata_path=_metadata_path,\n",
    "        query=_query,\n",
    "    ),\n",
    ")\n",
    "\n",
    "import kfp\n",
    "kfp.Client(host=cluster_endpoint).create_run_from_pipeline_package(\n",
    "    pipeline_file=_pipeline_name + '.tar.gz',\n",
    "    arguments={},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
